---
title: "EDX MovieLens Capstone Project"
author: Mark145
date: Summer 2019
output: 
  pdf_document:
   # toc: true
    #number_selections: true
fontsize: 12pt
geometry: margin=1in
---
```{r, setup, include=FALSE}
options(tinytex.verbose = TRUE)

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse, caret, data.table)
```

#introduction/overview/executive summary
an introduction/overview/executive summary section that describes the dataset and summarizes the goal of the project and key steps that were performed

Do you want insight into how internet based video services 'tailor' movie reccomendations specifically to you? Or do you want to know which aspects of your user data is being used to generate these recommendations?

Here you will be able to view the type of data used by video services and observe machine leaning techniques used to generate movie recommendations.



#methods/analysis
a methods/analysis section that explains the process and techniques used, such as data cleaning, data exploration and visualization, any insights gained, and your modeling approach
a results section that presents the modeling results and discusses the model performance

##aquire the data

The following script will generate 2 tables for us to analyze and model a video services machine learning algorithm against.  The two tables, edx set and validation set, are considered our train and test sets respectivly.

```{r, Create edx set, validation set}
################################
# Create edx set, validation set
################################

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data

set.seed(1)#, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

The data aquired from the following script is derived from the MovieLens 10M dataset.  The GroupLens stated summary of the data set is as follows:

"This data set contains 10000054 ratings and 95580 tags applied to 10681 movies by 71567 users of the online movie recommender service MovieLens.

Users were selected at random for inclusion. All users selected had rated at least 20 movies. Unlike previous MovieLens data sets, no demographic information is included. Each user is represented by an id, and no other information is provided." - http://files.grouplens.org/

##explore the data

Now we can explore our training data.  By examining the structure of our training and test sets.

```{r, set dims}
str(edx)
str(validation)
```

By looking at each set's structure, we see a slight difference in the dimensions.  'edx' has 9000055 objects where as 'validation' only has 999999 objects (sumed together = our expected MovieLens 10M data of 10000054).  Otherwise the data is setup in the same format, with varing values between the sets.  Below are some visual representations of the data.

```{r, set object sum}
dim(edx)[1]+dim(validation)[1]
```

Our ratings are stepped in .5 increments from .5 to 5 with 5 representing a movie for which the viewer had the most praise for and a .5 value rating representing a movie for which the viewer had the least praise.

```{r, rating chart}
ggplot(edx, aes(rating)) +
  geom_bar(fill = "darkgoldenrod") +
  ggtitle("Ratings Frequency")
```

Each movie had one or more associated genres.  In the plot below, the associated genres have been tallied and ordered with respect to their total associated count.  This count provides insight into the genre's the users in this data set first, care to rate, and second, each genre's prevalence.

The first insight, 'care to rate', was arrived at by the assumption that while each user was required to rate 20 movies, a user would not rate a movie that the user did not watch,  Therefore, the documented genre's prevalence for the data set can be considered valid.

```{r, genre separation}
genre_sep <- edx %>% separate_rows(genres, sep = "\\|") %>%
  select(genres) %>%
  group_by(genres) %>%
  summarise(count = n()) %>%
  arrange(desc(count))
genre_sep
```


```{r, genre exploration}
genre_sep %>%
  ggplot(aes(x = reorder(genres, -count), count)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  xlab("Genre") +
  ggtitle("Genre Frequency")

```
The last exploration of the data set that will be visualized is each movie's prevalence.

The chart below contains our movieIds along the x axis and the amount of times that movie was rated on the y axis.  It is save to say that more than 75% of the movies have less than 5,000 ratings.  Which could be valuable for start up video services.  Why host the full gamut of videos when the majority of the population only watch ~500 movies?  You can save on storage space, and work to optimize delivery of the top ~500 movies.

```{r, movie-prevalence}
title_count <- edx %>%
  select(movieId, title) %>%
  group_by(movieId) %>%
  summarise(count = n()) %>%
  arrange(desc(count))
title_count
```

```{r}
title_count %>%
  ggplot(aes(x = movieId, count)) +
  geom_violin(trim = TRUE, bw = 1000) +
  theme(axis.text = element_blank())
```

For this use case, we will focus on optimizing an algorithm for all movies.

Our check algorithm, how well we did will be computed by 'risdual mean square error' (RMSE).  We will find the difference for each user from our predicted rating for a movie, to what the user actually predicted.  Then square this value and average it for all users and movies.  Lastly, we will find the square root of that average and report the value.  A value of 1 means 1 rating value off form reality.

For example: userID 4 rated movieID 3 as a 4 rating.  We predicted (hopefully a 4) a 5 rating for userID4 on movieID3.  Our check function would work like this:
  5-4 = 1
  1^2 = 1
  mean(1) = 1
  sqrt(1) = 1
  RMSE = 1, not good.
  
```{r, RMSE function}
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```
  
First we will compute the average rating and call this mu.
```{r, rating average}
mu <- mean(edx$rating)
mu
```

With the average rating at 3.512 our base RMSE can now be computed for us guessing this average for every user in our test set.  The result of 1.06 below indicates we are off by a star if we always guess the average.
```{r}
RMSE(edx$rating, mu)
```

```{r}
naive_rmse <- RMSE(edx$rating, mu)
rmse_results <- data_frame(method = "Rating Average", RMSE = naive_rmse)
```
  
Now we will assume a movie can have bias (b_i).  We will compute the average rating on a movie by movie basis and compaire the difference of the moivie average to the overall average (1.06).  Then average this value of differences and add this into our recommendation algorithm.

```{r, movie avgs}
movie_avgs <- edx %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu))
movie_avgs

```

Now we can edit our recomendation algorithm by keeping our rating average and adding each movies average (as our bias).
```{r, add movie avgs}
predicted_ratings <- mu + edx %>%
  left_join(movie_avgs, by='movieId') %>%
  pull(b_i)
```



#conclusion
a conclusion section that gives a brief summary of the report, its limitations and future work (the last two are recommended but not necessary)